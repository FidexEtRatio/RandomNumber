The Shannon entropy is used with this formula: entropy = -numpy.sum(histogram * numpy.log2(histogram + 1e-10))
This implementation computes the Shannon entropy by:
    - multiplying each probability value (histogram) with the logarithm base 2 of that same probability value (numpy.log2(histogram))
    - summing these products across all possible values (pixel intensities)
    - negating the result to ensure a positive entropy value

Than it comes a question to mind: What is that 1e-10 doing in the formula, because the original formula has only the p(x) (pixel intensities) ...
Well the addition of 1e-10 is a safeguard to prevent mathematical issues when the histogram contains zero values (since we know that the logarithm of zero is undefined)

Now another big question: Why is it important?
Entropy helps in analyzing the complexity of randomness of a data. High entropy suggests more detail and variety, while low ones indicates uniformity
In our case the higher the entropy, the more "detailed" and diverse the image is in terms of pixel intensities